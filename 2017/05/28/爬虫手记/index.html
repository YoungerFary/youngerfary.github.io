<!DOCTYPE HTML>
<html lang="">
<head><meta name="generator" content="Hexo 3.8.0">
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
    <meta http-equiv="Cache-Control" content="no-siteapp">
    <meta http-equiv="Cache-Control" content="no-transform">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable" content="Hexo">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no,adress=no">
    <meta name="browsermode" content="application">
    <meta name="screen-orientation" content="portrait">
    <link rel="dns-prefetch" href="http://yoursite.com">
    <!--SEO-->





<meta name="robots" content="all">
<meta name="google" content="all">
<meta name="googlebot" content="all">
<meta name="verify" content="all">
    <!--Title-->


<title>爬虫手记 | Hexo</title>


    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">


    <link rel="icon" href="/favicon.ico">

    



<link rel="stylesheet" href="/css/bootstrap.min.css?rev=3.3.7">
<link rel="stylesheet" href="/css/font-awesome.min.css?rev=4.5.0">
<link rel="stylesheet" href="/css/style.css?rev=@@hash">




    
	<div class="hide">
		<script type="text/javascript">
			var cnzz_protocol = (("https:" == document.location.protocol) ? " https://" : " http://");document.write(unescape("%3Cspan class='cnzz_stat_icon_1263868967 hide' %3E%3Cscript%20src%3D%22https%3A%2F%2Fs95.cnzz.com%2Fz_stat.php%3Fweb_id%3D1272564536%22%3E%3C%2Fscript%3E%3C/span%3E%3Cscript src='" + cnzz_protocol + "s19.cnzz.com/z_stat.php%3Fid%3D1263868967%26show%3Dpic1' type='text/javascript'%3E%3C/script%3E"));
		</script>
	</div>






    

</head>

</html>
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<div style="ie">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<body>
    <header class="main-header" style="background-image:url(http://snippet.shenliyang.com/img/banner.jpg)">
    <div class="main-header-box">
        <a class="header-avatar" href="/" title="John Doe">
            <img src="/img/avatar.jpg" alt="logo头像" class="img-responsive center-block">
        </a>
        <div class="branding">
        	<!--<h2 class="text-hide">Snippet主题,从未如此简单有趣</h2>-->
            
                 <img src="/img/branding.png" alt="Snippet 博客主题" class="img-responsive center-block">
            
    	</div>
    </div>
</header>
    <nav class="main-navigation">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="navbar-header"><span class="nav-toggle-button collapsed pull-right" data-toggle="collapse" data-target="#main-menu" id="mnav">
                    <span class="sr-only"></span>
                        <i class="fa fa-bars"></i>
                    </span>
                    <a class="navbar-brand" href="http://yoursite.com">Hexo</a>
                </div>
                <div class="collapse navbar-collapse" id="main-menu">
                    <ul class="menu">
                        
                            <li role="presentation" class="text-center">
                                <a href="/"><i class="fa "></i>Home</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/前端/"><i class="fa "></i>前端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/后端/"><i class="fa "></i>后端</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/categories/工具/"><i class="fa "></i>工具</a>
                            </li>
                        
                            <li role="presentation" class="text-center">
                                <a href="/archives/"><i class="fa "></i>时间轴</a>
                            </li>
                        
                    </ul>
                </div>
            </div>
        </div>
    </div>
</nav>
    <section class="content-wrap">
        <div class="container">
            <div class="row">
                <main class="col-md-8 main-content m-post">
                    <p id="process"></p>
<article class="post">
    <div class="post-head">
        <h1 id="爬虫手记">
            
	            爬虫手记
            
        </h1>
        <div class="post-meta">
    
    
    <span class="categories-meta fa-wrap">
        <i class="fa fa-folder-open-o"></i>
        <a href="/categories/ ">
             
        </a>
    </span>
    

    
    <span class="fa-wrap">
        <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
                
                    <a href="/tags/python&amp;Scrapy" title="python&amp;Scrapy">
                        python&amp;Scrapy
                    </a>
                
            
        </span>
    </span>
    

    
        
        <span class="fa-wrap">
            <i class="fa fa-clock-o"></i>
            <span class="date-meta">2017/05/28</span>
        </span>
        
    
</div>

            
            
            <p class="fa fa-exclamation-triangle warning">
                <strong>543</strong>
            </p>
        
    </div>
    
    <div class="post-body post-content">
        <p><img src="http://ono5i4nh6.bkt.clouddn.com/%E7%88%AC%E8%99%AB.jpg" alt=""><br>最近写爬虫写的很开心，项目需求是要采集境外一些网站的关于PKK和巴基斯坦kill chinese people事件，途中也遇到很多问题，这篇博客主要是想记录一些问题，遇到的问题很多，也在慢慢找到感觉！我得去翻翻网页记录，一点点列出遇到的所有问题！<br><a id="more"></a></p>
<h2 id="一、VPN问题"><a href="#一、VPN问题" class="headerlink" title="一、VPN问题"></a>一、VPN问题</h2><p>今天有遇到一些问题，就是在爬虫写完之后，当我开始执行的时候，会出现以下错误：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DEBUG: Retrying &lt;GET https:<span class="comment">//www.voanews.com/s?k=Pakistan%2BChinese%2Bkidnapped&gt; (failed 2 times): TCP connection timed out: 10060: 由于连接方在一段时间后没有正确答复或连接的主机没有 反应，连接尝试失败。.</span></span><br></pre></td></tr></table></figure></p>
<p> 这个错误显示的是什么呢，他最简单的提示就是没有链接到我所要爬取得页面，为什么会这样呢？我开了vpn,在找了一番错误之后，发现如果用天行开了PPTP模式之后，他显示的线路是可以访问网页观看视屏，但是是禁止下载BT和一些东西的，我想是这个原因，可是后来改成高速模式之后依然不行！心慌慌，跑去问大神，大神说可以试试换个VPN，用shadowsocks开启全局模式，在尝试了之后，发现竟然成功了，膜拜啊！</p>
<h2 id="二、调试中的小错误"><a href="#二、调试中的小错误" class="headerlink" title="二、调试中的小错误"></a>二、调试中的小错误</h2><p>(1)这些错误都很小，但是为我整个过程积累了经验，所以我觉着有必要记录下来：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">KeyError: ‘Spider not found: voa_scrapy.py’</span><br></pre></td></tr></table></figure></p>
<p>这种错误典型的就是用scrapy crawl +爬虫名，这个爬虫名是程序中name后面定义的字符串，而不是简单的爬虫名，我的是name=’voa’，所以是crapy crawl voa<br>(2)windows下利用scrapy（python2.7）写爬虫，运行 scrapy crawl dmoz 命令时提示：exceptions.ImportError: No module named win32api<br>这没啥，主要是没安装pywin32，解决办法：安装pywin32<br>地址：！<a href="https://sourceforge.net/projects/pywin32/files/pywin32/Build%20220/" target="_blank" rel="noopener"></a><br>(3)遇到错误：DEBUG: Crawled (403) &lt;GET <a href="http://www.techbrood.com/&gt;" target="_blank" rel="noopener">http://www.techbrood.com/&gt;</a> (referer: None)<br>表示网站采用了防爬技术anti-web-crawling technique（Amazon所用），比较简单即会检查用户代理（User Agent）信息。<br>解决方法：<br>在请求头部构造一个User Agent，如下所示：<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">def <span class="title">start_requests</span><span class="params">(self)</span>:        </span></span><br><span class="line"><span class="function">yield <span class="title">Request</span><span class="params">(<span class="string">"http://www.techbrood.com/"</span>, headers=&#123;<span class="string">'User-Agent'</span>: <span class="string">"your agent string"</span>&#125;)</span></span></span><br></pre></td></tr></table></figure></p>
<p>(4)Python脚本运行出现语法错误：IndentationError: unindent does not match any outer indentation level<br>【解决过程】<br>1.对于此错误，最常见的原因是，的确没有对齐。但是我根据错误提示的行数，去代码中看了下，没啥问题啊。<br>都是用TAB键，对齐好了的，没有不对齐的行数啊。<br>2.以为是前面的注释的内容影响后面的语句的语法了，所以把前面的注释也删除了。<br>结果还是此语法错误。<br>3.后来折腾了半天，突然想到了，把当前python脚本的所有字符都显示出来看看有没有啥特殊的字符。</p>
<p>当前用的文本编辑器Notepad++，好像有个设置，可以显示所有的字符的。<br>找到了，在：<br>视图 -&gt; 显示符号 -&gt; 显示空格与制表符<br>这里可以看到tab键打出来的字符和空格打出来的字符是不一样的！那有什么方法是比较好取代的呢？去把对应的TAB，都改为空格，统一一下对齐的风格，即可。<br>在Notepad++中，去：<br>设置-&gt;首选项：语言-&gt;以空格取代（TAB键）：<br>即可实现，对于以后每次的TAB输入，都自动转换为4个空格。</p>
<h2 id="三、关于代理IP的获取和使用"><a href="#三、关于代理IP的获取和使用" class="headerlink" title="三、关于代理IP的获取和使用"></a>三、关于代理IP的获取和使用</h2><p>有时候在爬取某个网站的时候，用同一个IP去访问网站，网站会认为这个是机器人，就会对这个ip进行一个ip限制，所以我们得用不同的ip去爬！那怎样去操作呢！</p>
<h3 id="1、手动更新IP池"><a href="#1、手动更新IP池" class="headerlink" title="1、手动更新IP池"></a>1、手动更新IP池</h3><p>步骤如下：</p>
<h4 id="（1）在settings配置文件中新增IP池"><a href="#（1）在settings配置文件中新增IP池" class="headerlink" title="（1）在settings配置文件中新增IP池:"></a>（1）在settings配置文件中新增IP池:</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">IPPOOL=[  </span><br><span class="line">    &#123;<span class="string">"ipaddr"</span>:<span class="string">"61.129.70.131:8080"</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">"ipaddr"</span>:<span class="string">"61.152.81.193:9100"</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">"ipaddr"</span>:<span class="string">"120.204.85.29:3128"</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">"ipaddr"</span>:<span class="string">"219.228.126.86:8123"</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">"ipaddr"</span>:<span class="string">"61.152.81.193:9100"</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">"ipaddr"</span>:<span class="string">"218.82.33.225:53853"</span>&#125;,  </span><br><span class="line">    &#123;<span class="string">"ipaddr"</span>:<span class="string">"223.167.190.17:42789"</span>&#125;  </span><br><span class="line">]</span><br></pre></td></tr></table></figure>
<p>这些IP可以从这个几个网站获取:快代理、代理66、有代理、西刺代理、guobanjia。如果出现像下面这种提示:”由于连接方在一段时间后没有正确答复或连接的主机没有反应，连接尝试失败”或者是这种，” 由 于目标计算机积极拒绝，无法连接。”. 那就是IP的问题，更换就行了,这也有可能是我们上面说的原因！突然发现上面好多IP都不能用。如果需要，你可以自己去代理网站上去查！<br>在Scrapy中与代理服务器设置相关的下载中间件是HttpProxyMiddleware，对应的类为:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware</span><br></pre></td></tr></table></figure></p>
<h4 id="（2）修改中间件文件middlewares-py"><a href="#（2）修改中间件文件middlewares-py" class="headerlink" title="（2）修改中间件文件middlewares.py"></a>（2）修改中间件文件middlewares.py</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-  </span><br><span class="line"></span><br><span class="line"># Define here the models for your spider middleware  </span><br><span class="line">#  </span><br><span class="line"># See documentation in:  </span><br><span class="line"># http://doc.scrapy.org/en/latest/topics/spider-middleware.html  </span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> random  </span><br><span class="line">from scrapy <span class="keyword">import</span> signals  </span><br><span class="line">from myproxies.settings <span class="keyword">import</span> IPPOOL  </span><br><span class="line"></span><br><span class="line">class MyproxiesSpiderMiddleware(object):  </span><br><span class="line"></span><br><span class="line">      <span class="function">def <span class="title">__init__</span><span class="params">(self,ip=<span class="string">''</span>)</span>:  </span></span><br><span class="line"><span class="function">          self.ip</span>=ip  </span><br><span class="line"></span><br><span class="line">      <span class="function">def <span class="title">process_request</span><span class="params">(self, request, spider)</span>:  </span></span><br><span class="line"><span class="function">          thisip</span>=random.choice(IPPOOL)  </span><br><span class="line">          print(<span class="string">"this is ip:"</span>+thisip[<span class="string">"ipaddr"</span>])  </span><br><span class="line">          request.meta[<span class="string">"proxy"</span>]=<span class="string">"http://"</span>+thisip[<span class="string">"ipaddr"</span>]</span><br></pre></td></tr></table></figure>
<p>这里就是将IPPOOL中的ip通过request传送出去！</p>
<h4 id="（3）在settings中设置DOWNLOADER-MIDDLEWARES"><a href="#（3）在settings中设置DOWNLOADER-MIDDLEWARES" class="headerlink" title="（3）在settings中设置DOWNLOADER_MIDDLEWARES"></a>（3）在settings中设置DOWNLOADER_MIDDLEWARES</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;  </span><br><span class="line">#    'myproxies.middlewares.MyCustomDownloaderMiddleware': 543,  </span><br><span class="line">     <span class="string">'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware'</span>:<span class="number">543</span>,  </span><br><span class="line">     <span class="string">'myproxies.middlewares.MyproxiesSpiderMiddleware'</span>:<span class="number">125</span>  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="4-爬虫文件为"><a href="#4-爬虫文件为" class="headerlink" title="(4)爬虫文件为"></a>(4)爬虫文件为</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># -*- coding: utf-8 -*-  </span><br><span class="line"><span class="keyword">import</span> scrapy  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class ProxieSpider(scrapy.Spider):  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">__init__</span><span class="params">(self)</span>:  </span></span><br><span class="line"><span class="function">        self.headers </span>= &#123;  </span><br><span class="line">            <span class="string">'Content-Type'</span>:<span class="string">'application/x-www-form-urlencoded; charset=UTF-8'</span>,  </span><br><span class="line">            <span class="string">'Accept-Encoding'</span>:<span class="string">'gzip, deflate'</span>,  </span><br><span class="line">            <span class="string">'User-Agent'</span>:<span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'</span>  </span><br><span class="line">        &#125;  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    name = <span class="string">"proxie"</span>  </span><br><span class="line">    allowed_domains = [<span class="string">"sina.com.cn"</span>]  </span><br><span class="line">    start_urls = [<span class="string">'http://news.sina.com.cn/'</span>]  </span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">parse</span><span class="params">(self, response)</span>:  </span></span><br><span class="line"><span class="function">        <span class="title">print</span><span class="params">(response.body)</span></span></span><br></pre></td></tr></table></figure>
<h4 id="（5）运行爬虫"><a href="#（5）运行爬虫" class="headerlink" title="（5）运行爬虫"></a>（5）运行爬虫</h4><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl proxie</span><br></pre></td></tr></table></figure>
<h3 id="2、自动更新IP池"><a href="#2、自动更新IP池" class="headerlink" title="2、自动更新IP池"></a>2、自动更新IP池</h3><p>有时候可能随机的ip池中的ip还是不行，我们得写个程序去哪弄点ip过来！<br>(1)这里写个自动获取IP的类proxies.py，执行一下把获取的IP保存到txt文件中去:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"># *-* coding:utf-8 *-*  </span><br><span class="line"><span class="keyword">import</span> requests  </span><br><span class="line">from bs4 <span class="keyword">import</span> BeautifulSoup  </span><br><span class="line"><span class="keyword">import</span> lxml  </span><br><span class="line">from multiprocessing <span class="keyword">import</span> Process, Queue  </span><br><span class="line"><span class="keyword">import</span> random  </span><br><span class="line"><span class="keyword">import</span> json  </span><br><span class="line"><span class="keyword">import</span> time  </span><br><span class="line"><span class="keyword">import</span> requests  </span><br><span class="line"></span><br><span class="line">class Proxies(object):  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="string">""</span><span class="string">"docstring for Proxies"</span><span class="string">""</span>  </span><br><span class="line">    <span class="function">def <span class="title">__init__</span><span class="params">(self, page=<span class="number">3</span>)</span>:  </span></span><br><span class="line"><span class="function">        self.proxies </span>= []  </span><br><span class="line">        self.verify_pro = []  </span><br><span class="line">        self.page = page  </span><br><span class="line">        self.headers = &#123;  </span><br><span class="line">        <span class="string">'Accept'</span>: <span class="string">'*//*'</span>,  </span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/45.0.2454.101 Safari/537.36'</span>,  </span><br><span class="line">        <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate, sdch'</span>,  </span><br><span class="line">        <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.8'</span>  </span><br><span class="line">        &#125;  </span><br><span class="line">        self.get_proxies()  </span><br><span class="line">        self.get_proxies_nn()  </span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">get_proxies</span><span class="params">(self)</span>:  </span></span><br><span class="line"><span class="function">        page </span>= random.randint(<span class="number">1</span>,<span class="number">10</span>)  </span><br><span class="line">        page_stop = page + self.page  </span><br><span class="line">        <span class="keyword">while</span> page &lt; page_stop:  </span><br><span class="line">            url = <span class="string">'http://www.xicidaili.com/nt/%d'</span> % page  </span><br><span class="line">            html = requests.get(url, headers=self.headers).content  </span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">'lxml'</span>)  </span><br><span class="line">            ip_list = soup.find(id=<span class="string">'ip_list'</span>)  </span><br><span class="line">            <span class="keyword">for</span> odd in ip_list.find_all(class_=<span class="string">'odd'</span>):  </span><br><span class="line">                protocol = odd.find_all(<span class="string">'td'</span>)[<span class="number">5</span>].get_text().lower()+<span class="string">'://'</span>  </span><br><span class="line">                self.proxies.append(protocol + <span class="string">':'</span>.join([x.get_text() <span class="keyword">for</span> x in odd.find_all(<span class="string">'td'</span>)[<span class="number">1</span>:<span class="number">3</span>]]))  </span><br><span class="line">            page += <span class="number">1</span>  </span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">get_proxies_nn</span><span class="params">(self)</span>:  </span></span><br><span class="line"><span class="function">        page </span>= random.randint(<span class="number">1</span>,<span class="number">10</span>)  </span><br><span class="line">        page_stop = page + self.page  </span><br><span class="line">        <span class="keyword">while</span> page &lt; page_stop:  </span><br><span class="line">            url = <span class="string">'http://www.xicidaili.com/nn/%d'</span> % page  </span><br><span class="line">            html = requests.get(url, headers=self.headers).content  </span><br><span class="line">            soup = BeautifulSoup(html, <span class="string">'lxml'</span>)  </span><br><span class="line">            ip_list = soup.find(id=<span class="string">'ip_list'</span>)  </span><br><span class="line">            <span class="keyword">for</span> odd in ip_list.find_all(class_=<span class="string">'odd'</span>):  </span><br><span class="line">                protocol = odd.find_all(<span class="string">'td'</span>)[<span class="number">5</span>].get_text().lower() + <span class="string">'://'</span>  </span><br><span class="line">                self.proxies.append(protocol + <span class="string">':'</span>.join([x.get_text() <span class="keyword">for</span> x in odd.find_all(<span class="string">'td'</span>)[<span class="number">1</span>:<span class="number">3</span>]]))  </span><br><span class="line">            page += <span class="number">1</span>  </span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">verify_proxies</span><span class="params">(self)</span>:  </span></span><br><span class="line"><span class="function">        # 没验证的代理  </span></span><br><span class="line"><span class="function">        old_queue </span>= Queue()  </span><br><span class="line">        # 验证后的代理  </span><br><span class="line">        new_queue = Queue()  </span><br><span class="line">        print (<span class="string">'verify proxy........'</span>)  </span><br><span class="line">        works = []  </span><br><span class="line">        <span class="function"><span class="keyword">for</span> _ in <span class="title">range</span><span class="params">(<span class="number">15</span>)</span>:  </span></span><br><span class="line"><span class="function">            works.<span class="title">append</span><span class="params">(Process(target=self.verify_one_proxy, args=(old_queue,new_queue)</span>))  </span></span><br><span class="line"><span class="function">        <span class="keyword">for</span> work in works:  </span></span><br><span class="line"><span class="function">            work.<span class="title">start</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function">        <span class="keyword">for</span> proxy in self.proxies:  </span></span><br><span class="line"><span class="function">            old_queue.<span class="title">put</span><span class="params">(proxy)</span>  </span></span><br><span class="line"><span class="function">        <span class="keyword">for</span> work in works:  </span></span><br><span class="line"><span class="function">            old_queue.<span class="title">put</span><span class="params">(<span class="number">0</span>)</span>  </span></span><br><span class="line"><span class="function">        <span class="keyword">for</span> work in works:  </span></span><br><span class="line"><span class="function">            work.<span class="title">join</span><span class="params">()</span>  </span></span><br><span class="line"><span class="function">        self.proxies </span>= []  </span><br><span class="line">        <span class="keyword">while</span> <span class="number">1</span>:  </span><br><span class="line">            <span class="keyword">try</span>:  </span><br><span class="line">                self.proxies.append(new_queue.get(timeout=<span class="number">1</span>))  </span><br><span class="line">            except:  </span><br><span class="line">                <span class="keyword">break</span>  </span><br><span class="line">        print (<span class="string">'verify_proxies done!'</span>)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">verify_one_proxy</span><span class="params">(self, old_queue, new_queue)</span>:  </span></span><br><span class="line"><span class="function">        <span class="keyword">while</span> 1:  </span></span><br><span class="line"><span class="function">            proxy </span>= old_queue.get()  </span><br><span class="line">            <span class="keyword">if</span> proxy == <span class="number">0</span>:<span class="keyword">break</span>  </span><br><span class="line">            protocol = <span class="string">'https'</span> <span class="keyword">if</span> <span class="string">'https'</span> in proxy <span class="keyword">else</span> <span class="string">'http'</span>  </span><br><span class="line">            proxies = &#123;protocol: proxy&#125;  </span><br><span class="line">            <span class="keyword">try</span>:  </span><br><span class="line">                <span class="keyword">if</span> requests.get(<span class="string">'http://www.baidu.com'</span>, proxies=proxies, timeout=<span class="number">2</span>).status_code == <span class="number">200</span>:  </span><br><span class="line">                    print (<span class="string">'success %s'</span> % proxy)  </span><br><span class="line">                    new_queue.put(proxy)  </span><br><span class="line">            except:  </span><br><span class="line">                print (<span class="string">'fail %s'</span> % proxy)  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:  </span><br><span class="line">    a = Proxies()  </span><br><span class="line">    a.verify_proxies()  </span><br><span class="line">    print (a.proxies)  </span><br><span class="line">    proxie = a.proxies   </span><br><span class="line">    <span class="function">with <span class="title">open</span><span class="params">(<span class="string">'proxies.txt'</span>, <span class="string">'a'</span>)</span> as f:  </span></span><br><span class="line"><span class="function">       <span class="keyword">for</span> proxy in proxie:  </span></span><br><span class="line"><span class="function">             f.<span class="title">write</span><span class="params">(proxy+<span class="string">'\n'</span>)</span></span></span><br></pre></td></tr></table></figure></p>
<p>执行一下:  Python  proxies.py     这些IP就会保存到proxies.txt文件中去</p>
<p>(2)修改代理文件middlewares.py的内容为如下:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random  </span><br><span class="line"><span class="keyword">import</span> scrapy  </span><br><span class="line">from scrapy <span class="keyword">import</span> log  </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># logger = logging.getLogger()  </span><br><span class="line"></span><br><span class="line">class ProxyMiddleWare(object):  </span><br><span class="line">    <span class="string">""</span><span class="string">"docstring for ProxyMiddleWare"</span><span class="string">""</span>  </span><br><span class="line">    <span class="function">def <span class="title">process_request</span><span class="params">(self,request, spider)</span>:  </span></span><br><span class="line"><span class="function">        '''对request对象加上proxy'''  </span></span><br><span class="line"><span class="function">        proxy </span>= self.get_random_proxy()  </span><br><span class="line">        print(<span class="string">"this is request ip:"</span>+proxy)  </span><br><span class="line">        request.meta[<span class="string">'proxy'</span>] = proxy   </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">process_response</span><span class="params">(self, request, response, spider)</span>:  </span></span><br><span class="line"><span class="function">        '''对返回的response处理'''  </span></span><br><span class="line"><span class="function">        # 如果返回的response状态不是200，重新生成当前request对象  </span></span><br><span class="line"><span class="function">        <span class="keyword">if</span> response.status !</span>= <span class="number">200</span>:  </span><br><span class="line">            proxy = self.get_random_proxy()  </span><br><span class="line">            print(<span class="string">"this is response ip:"</span>+proxy)  </span><br><span class="line">            # 对当前reque加上代理  </span><br><span class="line">            request.meta[<span class="string">'proxy'</span>] = proxy   </span><br><span class="line">            <span class="keyword">return</span> request  </span><br><span class="line">        <span class="keyword">return</span> response  </span><br><span class="line"></span><br><span class="line">    <span class="function">def <span class="title">get_random_proxy</span><span class="params">(self)</span>:  </span></span><br><span class="line"><span class="function">        '''随机从文件中读取proxy'''  </span></span><br><span class="line"><span class="function">        <span class="keyword">while</span> 1:  </span></span><br><span class="line"><span class="function">            with <span class="title">open</span><span class="params">(<span class="string">'G:\\Scrapy_work\\myproxies\\myproxies\\proxies.txt'</span>, <span class="string">'r'</span>)</span> as f:  </span></span><br><span class="line"><span class="function">                proxies </span>= f.readlines()  </span><br><span class="line">            <span class="keyword">if</span> proxies:  </span><br><span class="line">                <span class="keyword">break</span>  </span><br><span class="line">            <span class="keyword">else</span>:  </span><br><span class="line">                time.sleep(<span class="number">1</span>)  </span><br><span class="line">        proxy = random.choice(proxies).strip()  </span><br><span class="line">        <span class="keyword">return</span> proxy</span><br></pre></td></tr></table></figure></p>
<p>(3)修改下settings文件:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">DOWNLOADER_MIDDLEWARES = &#123;  </span><br><span class="line">#    'myproxies.middlewares.MyCustomDownloaderMiddleware': 543,  </span><br><span class="line">     <span class="string">'scrapy.contrib.downloadermiddleware.httpproxy.HttpProxyMiddleware'</span>:None,  </span><br><span class="line">     <span class="string">'myproxies.middlewares.ProxyMiddleWare'</span>:<span class="number">125</span>,  </span><br><span class="line">     <span class="string">'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware'</span>:None  </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>(4)运行爬虫:<br><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl proxie</span><br></pre></td></tr></table></figure></p>
<p>这样就可以了！哈哈哈</p>
<p>多走路，多思考，越努力，越幸运！<br>                                                                                                                            ———————————————YoungerFary</p>

    </div>
    
        <div class="reward">
    <div class="reward-wrap">赏
        <div class="reward-box">
            
            
                <span class="reward-type">
                    <img class="wechat" src="../img/reward-wepay.jpg"><b>微信打赏</b>
                </span>
            
        </div>
    </div>
    <p class="reward-tip">赞赏是不耍流氓的鼓励</p>
</div>


    
    <div class="post-footer">
        <div>
            
                转载声明：商业转载请联系作者获得授权,非商业转载请注明出处 © <a href="" target="_blank">Snippet</a>
            
        </div>
        <div>
            
        </div>
    </div>
</article>

<div class="article-nav prev-next-wrap clearfix">
    
        <a href="/2017/05/29/爬虫系列（一）/" class="pre-post btn btn-default" title="爬虫系列（一）">
            <i class="fa fa-angle-left fa-fw"></i><span class="hidden-lg">上一篇</span>
            <span class="hidden-xs">爬虫系列（一）</span>
        </a>
    
    
        <a href="/2017/05/16/SSM之MyBatis（五）/" class="next-post btn btn-default" title="SSM之MyBatis（五）">
            <span class="hidden-lg">下一篇</span>
            <span class="hidden-xs">SSM之MyBatis（五）</span><i class="fa fa-angle-right fa-fw"></i>
        </a>
    
</div>


    <div id="comments">
        
    
    <div id="vcomments" class="valine"></div>
    <script src="//cdn1.lncld.net/static/js/3.0.4/av-min.js"></script>
<script src="/assets/valine.min.js"></script>

    <script>
        new Valine({
            av: AV,
            el: '#vcomments',
            appId: 'xOKV9J4UeQAtVkvnJC7Kq2Jn-gzGzoHsz',
            appKey: 'erIpQac4azoCmgfBB7Dl9maa',
            placeholder: '说点什么吧',
            notify: false,
            verify: false,
            avatar: 'mm',
            meta: 'nick,mail'.split(','),
            pageSize: '10',
            path: window.location.pathname,
            lang: ''.toLowerCase()
        })
    </script>


    </div>





                </main>
                
                    <aside id="article-toc" role="navigation" class="col-md-4">
    <div class="widget">
        <h3 class="title">Table of Contents</h3>
        
            <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#一、VPN问题"><span class="toc-text">一、VPN问题</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#二、调试中的小错误"><span class="toc-text">二、调试中的小错误</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#三、关于代理IP的获取和使用"><span class="toc-text">三、关于代理IP的获取和使用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1、手动更新IP池"><span class="toc-text">1、手动更新IP池</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#（1）在settings配置文件中新增IP池"><span class="toc-text">（1）在settings配置文件中新增IP池:</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（2）修改中间件文件middlewares-py"><span class="toc-text">（2）修改中间件文件middlewares.py</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（3）在settings中设置DOWNLOADER-MIDDLEWARES"><span class="toc-text">（3）在settings中设置DOWNLOADER_MIDDLEWARES</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#4-爬虫文件为"><span class="toc-text">(4)爬虫文件为</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#（5）运行爬虫"><span class="toc-text">（5）运行爬虫</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2、自动更新IP池"><span class="toc-text">2、自动更新IP池</span></a></li></ol></li></ol>
        
    </div>
</aside>

                
            </div>
        </div>
    </section>
    <footer class="main-footer">
    <div class="container">
        <div class="row">
        </div>
    </div>
</footer>

<a id="back-to-top" class="icon-btn hide">
	<i class="fa fa-chevron-up"></i>
</a>




    <div class="copyright">
    <div class="container">
        <div class="row">
            <div class="col-sm-12">
                <div class="busuanzi">
    
</div>

            </div>
            <div class="col-sm-12">
                <span>Copyright &copy; 2017
                </span> |
                <span>
                    Powered by <a href="//hexo.io" class="copyright-links" target="_blank" rel="nofollow">Hexo</a>
                </span> |
                <span>
                    Theme by <a href="//github.com/shenliyang/hexo-theme-snippet.git" class="copyright-links" target="_blank" rel="nofollow">Snippet</a>
                </span>
            </div>
        </div>
    </div>
</div>







<script src="/js/app.js?rev=@@hash"></script>

</body>
</html>